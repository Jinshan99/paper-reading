{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe2dc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from tensorflow.keras.applications.vgg16 import VGG16,preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e04ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617464e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR='dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d407d86",
   "metadata": {},
   "source": [
    "## Extract image features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad41214f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load vgg16 model\n",
    "model=VGG16()\n",
    "#RESTRUCTURE THE MODEL\n",
    "model=Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "# we don't need to fully connect layer, we need the previous layer in order to extract the feature results, leaving the last layer and getting the before layer output and assigning it to outputs\n",
    "#summarize\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8176ec7f",
   "metadata": {},
   "source": [
    " ### outputs=model.layers[-1].output\n",
    " fc1 (Dense)                 (None, 4096)              102764544 \n",
    "                                                                 \n",
    " fc2 (Dense)                 (None, 4096)              16781312  \n",
    "                                                                 \n",
    " predictions (Dense)         (None, 1000)              4097000  \n",
    " we don't need the predictions layer, so outputs=model.layers[-2].output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08ffe96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract features from image\n",
    "features={}\n",
    "directory=os.path.join(BASE_DIR,'images')\n",
    "\n",
    "for img_name in tqdm(os.listdir(directory)):\n",
    "    #load image from file\n",
    "    img_path=directory+'/'+img_name\n",
    "    image=load_img(img_path,target_size=(224,224))\n",
    "    #convert image pixels to numpy array\n",
    "    image=img_to_array(image)\n",
    "    #reshape data for model\n",
    "    image=image.reshape((1,image.shape[0],image.shape[1],image.shape[2]))#RGB,has 3 dim\n",
    "    #preprocess image for vgg\n",
    "    image=preprocess_input(image)\n",
    "    #extract features\n",
    "    feature=model.predict(image,verbose=0)\n",
    "    #get image id\n",
    "    image_id=img_name.split('.')[0]#****.jpg, get ****\n",
    "    #store feature\n",
    "    features[image_id]=feature\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441d0546",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKING_DIR='working'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824870ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#store features in pickle\n",
    "pickle.dump(features,open(os.path.join(WORKING_DIR,'features.pkl'),'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee44f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load features from pickle\n",
    "with open(os.path.join(WORKING_DIR,'features.pkl'),'rb')as f:\n",
    "    features=pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcad1d55",
   "metadata": {},
   "source": [
    "## load the Captions Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a723ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(BASE_DIR,'captions.txt'),'r')as f:\n",
    "    next(f)\n",
    "    captions_doc =f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f733846",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24eefb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create mapping of image to captions\n",
    "mapping={}\n",
    "#process lines\n",
    "for line in tqdm(captions_doc.split('\\n')):\n",
    "    #split the line by comma(,)\n",
    "    tokens=line.split(',')\n",
    "    if len(line)<2:\n",
    "        continue\n",
    "    image_id,caption=tokens[0],tokens[1:]#1: because A child in a pink dress is climbing up.. has lot columns\n",
    "    #remove extension from image id\n",
    "    image_id=image_id.split('.')[0]\n",
    "    #convert caption list to string\n",
    "    caption=''.join(caption)\n",
    "    #create list if needed\n",
    "    if image_id not in mapping:\n",
    "        mapping[image_id]=[]\n",
    "    #store the caption\n",
    "    mapping[image_id].append(caption)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69af7430",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mapping)#we do have all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3fec12",
   "metadata": {},
   "source": [
    "## Preprocess text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737be1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(mapping):\n",
    "    for key, captions in mapping.items():\n",
    "        #key is the image\n",
    "        for i in range(len(captions)):\n",
    "            #take one caption at a time\n",
    "            caption=captions[i]\n",
    "            #preprocessing steps\n",
    "            #convert to lowercase\n",
    "            caption=caption.lower()\n",
    "            #delete digits, special chars etc\n",
    "            caption=caption.replace('[^A-Za-z]','')#remove everything except alpha\n",
    "            caption=caption.replace('\\s+',' ')#instead multiple space, to single space\n",
    "            #add start and end tags to the caption\n",
    "            caption='<start>'+\" \".join([word for word in caption.split() if len(word)>1])+'<end>'\n",
    "            captions[i]=caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f4adca",
   "metadata": {},
   "outputs": [],
   "source": [
    "##before preprecess of text\n",
    "mapping['1000268201_693b08cb0e']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dc98a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing the text\n",
    "clean(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5c7eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#after preprocessing the text\n",
    "mapping['1000268201_693b08cb0e']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36436189",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_captions=[]\n",
    "for key in mapping:\n",
    "    for caption in mapping[key]:\n",
    "        all_captions.append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e4b54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12407a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_captions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e567c114",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize the text\n",
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts(all_captions)\n",
    "vocab_size=len(tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a4d107",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37186207",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get maximum length of the caption available\n",
    "max_length=max(len(caption.split()) for caption in all_captions)\n",
    "max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048052de",
   "metadata": {},
   "source": [
    "## Train Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d61f3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ids=list(mapping.keys())\n",
    "split=int(len(image_ids)*0.90)\n",
    "train=image_ids[:split]\n",
    "test=image_ids[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874e0dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#<start>girl going into wooden building<end>\n",
    "#        X                   y\n",
    "#<start>                    girl\n",
    "#<start>girl                going   \n",
    "#<start>girl going          into \n",
    "#.......\n",
    "#<start>girl going into wooden building      <end>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5209fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create data generator to get data in batch (avoids session crash)\n",
    "def data_generator(data_keys, mapping, features, tokenizer, max_length, vocab_size, batch_size):\n",
    "    #loop over images\n",
    "    X1,X2,y=list(),list(),list()\n",
    "    n=0\n",
    "    while 1:\n",
    "        for key in data_keys:\n",
    "            n+=1\n",
    "            captions=mapping[key]\n",
    "            #process each caption\n",
    "            for caption in captions:\n",
    "                #encode the sequence\n",
    "                seq=tokenizer.texts_to_sequence([caption])[0]\n",
    "                #split the sequence into X, y pairs\n",
    "                for i in range(1, len(seq)):\n",
    "                    #split into input and output pairs\n",
    "                    in_seq, out_seq=seq[:i],seq[i]\n",
    "                    #pad input sequence\n",
    "                    in_seq=pad_sequences([in_seq],maxlen=max_length)[0] \n",
    "                    #encode output sequence\n",
    "                    out_seq=to_categorical([out_seq],num_classes=vocal_size)[0]\n",
    "                    \n",
    "                    #store the sequences\n",
    "                    X1.append(features[key][0])\n",
    "                    X2.append(in_seq)\n",
    "                    y.append(out_seq)\n",
    "            if n == batch_size:\n",
    "                X1,X2,y=np.array(X1),np.array(X2),np.array(y)\n",
    "                yield[X1,X2],y\n",
    "                X1,X2,y=list(),list(),list()\n",
    "                n=0\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecf8496",
   "metadata": {},
   "source": [
    "## Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14487cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a4007f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder model\n",
    "#image feature layers\n",
    "inputs1=Input(shape=(4096,))\n",
    "fe1=Dropout(0.4)(inputs1)\n",
    "fe2=Dense(256,activation='relu')(fe1)\n",
    "#sequence feature layers\n",
    "inputs2=Input(shape=(max_length,))\n",
    "se1=Embedding(vocab_size,256,mask_zero=True)(inputs2)\n",
    "se2=Dropout(0.4)(se1)\n",
    "se3=LSTM(256)(se2)\n",
    "\n",
    "#decoder model\n",
    "decoder1=add([fe2,se3])\n",
    "decoder2=Dense(256,activation='relu')(decoder1)\n",
    "outputs=Dense(vocab_size,activation='softmax')(decoder2)\n",
    "\n",
    "model=Model(inputs=[inputs1,inputs2],outputs=outputs)\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam')\n",
    "\n",
    "#plot the model\n",
    "plot_model(model,show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e5c6f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee266905",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
